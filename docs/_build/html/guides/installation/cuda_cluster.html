

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CUDA Cluster Installation Guide &mdash; GeminiSDR 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=8d563738"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Troubleshooting Guide" href="../troubleshooting.html" />
    <link rel="prev" title="Linux VM Installation Guide" href="linux_vm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            GeminiSDR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">User Guides</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Installation Guides</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="m1_mac.html">M1 Mac Installation Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="linux_vm.html">Linux VM Installation Guide</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">CUDA Cluster Installation Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installation-steps">Installation Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-gpu-configuration">Multi-GPU Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performance-optimization">Performance Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-and-profiling">Monitoring and Profiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster-job-management">Cluster Job Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-and-validation">Testing and Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="#best-practices">Best Practices</a></li>
<li class="toctree-l4"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#quick-platform-selection">Quick Platform Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#detailed-installation-guides">Detailed Installation Guides</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../troubleshooting.html">Troubleshooting Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../performance_optimization.html">Performance Optimization Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#installation-guides">Installation Guides</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#essential-guides">Essential Guides</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#quick-start">Quick Start</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/index.html">System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../development/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/index.html">Examples and Tutorials</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">GeminiSDR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">User Guides</a></li>
          <li class="breadcrumb-item"><a href="index.html">Installation Guides</a></li>
      <li class="breadcrumb-item active">CUDA Cluster Installation Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/guides/installation/cuda_cluster.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="cuda-cluster-installation-guide">
<h1>CUDA Cluster Installation Guide<a class="headerlink" href="#cuda-cluster-installation-guide" title="Link to this heading"></a></h1>
<p>This guide provides detailed installation instructions for GeminiSDR on CUDA-enabled clusters and high-performance computing environments.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h2>
<section id="system-requirements">
<h3>System Requirements<a class="headerlink" href="#system-requirements" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Hardware</strong>:</p>
<ul>
<li><p>NVIDIA GPU with CUDA Compute Capability 6.0+ (Pascal architecture or newer)</p></li>
<li><p>16GB+ system RAM (32GB+ recommended)</p></li>
<li><p>8GB+ GPU memory per GPU (16GB+ recommended)</p></li>
<li><p>High-speed storage (NVMe SSD recommended)</p></li>
</ul>
</li>
<li><p><strong>Software</strong>:</p>
<ul>
<li><p>Linux distribution (Ubuntu 20.04 LTS, CentOS 8+, or RHEL 8+)</p></li>
<li><p>NVIDIA GPU drivers (470.x or later)</p></li>
<li><p>CUDA Toolkit 11.8 or later</p></li>
<li><p>cuDNN 8.6 or later</p></li>
<li><p>Python 3.9+</p></li>
</ul>
</li>
<li><p><strong>Network</strong>:</p>
<ul>
<li><p>High-bandwidth network for multi-node setups</p></li>
<li><p>InfiniBand or 10GbE recommended for distributed training</p></li>
</ul>
</li>
</ul>
</section>
<section id="cluster-environment-setup">
<h3>Cluster Environment Setup<a class="headerlink" href="#cluster-environment-setup" title="Link to this heading"></a></h3>
<p><strong>SLURM Integration</strong> (if applicable):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check SLURM availability</span>
sinfo
squeue

<span class="c1"># Check available GPUs</span>
sinfo<span class="w"> </span>-o<span class="w"> </span><span class="s2">&quot;%N %G&quot;</span>
</pre></div>
</div>
<p><strong>Module System</strong> (if applicable):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load required modules</span>
module<span class="w"> </span>load<span class="w"> </span>cuda/11.8
module<span class="w"> </span>load<span class="w"> </span>python/3.9
module<span class="w"> </span>load<span class="w"> </span>gcc/9.3.0
</pre></div>
</div>
</section>
</section>
<section id="installation-steps">
<h2>Installation Steps<a class="headerlink" href="#installation-steps" title="Link to this heading"></a></h2>
<section id="verify-cuda-installation">
<h3>1. Verify CUDA Installation<a class="headerlink" href="#verify-cuda-installation" title="Link to this heading"></a></h3>
<p>Check CUDA and GPU availability:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check NVIDIA driver</span>
nvidia-smi

<span class="c1"># Check CUDA version</span>
nvcc<span class="w"> </span>--version

<span class="c1"># Check GPU compute capability</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>compute_cap<span class="w"> </span>--format<span class="o">=</span>csv

<span class="c1"># Test CUDA samples (if available)</span>
<span class="nb">cd</span><span class="w"> </span>/usr/local/cuda/samples/1_Utilities/deviceQuery
make<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./deviceQuery
</pre></div>
</div>
</section>
<section id="set-up-python-environment">
<h3>2. Set Up Python Environment<a class="headerlink" href="#set-up-python-environment" title="Link to this heading"></a></h3>
<p>Create isolated Python environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create virtual environment</span>
python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>geminisdr-cuda-env

<span class="c1"># Activate environment</span>
<span class="nb">source</span><span class="w"> </span>geminisdr-cuda-env/bin/activate

<span class="c1"># Upgrade pip</span>
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip<span class="w"> </span>wheel<span class="w"> </span>setuptools
</pre></div>
</div>
</section>
<section id="install-cuda-enabled-pytorch">
<h3>3. Install CUDA-Enabled PyTorch<a class="headerlink" href="#install-cuda-enabled-pytorch" title="Link to this heading"></a></h3>
<p>Install PyTorch with CUDA support:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install PyTorch with CUDA 11.8 support</span>
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu118

<span class="c1"># Verify CUDA availability</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;</span>
<span class="s2">import torch</span>
<span class="s2">print(f&#39;PyTorch version: {torch.__version__}&#39;)</span>
<span class="s2">print(f&#39;CUDA available: {torch.cuda.is_available()}&#39;)</span>
<span class="s2">print(f&#39;CUDA version: {torch.version.cuda}&#39;)</span>
<span class="s2">print(f&#39;GPU count: {torch.cuda.device_count()}&#39;)</span>
<span class="s2">for i in range(torch.cuda.device_count()):</span>
<span class="s2">    print(f&#39;GPU {i}: {torch.cuda.get_device_name(i)}&#39;)</span>
<span class="s2">&quot;</span>
</pre></div>
</div>
</section>
<section id="install-system-dependencies">
<h3>4. Install System Dependencies<a class="headerlink" href="#install-system-dependencies" title="Link to this heading"></a></h3>
<p><strong>Ubuntu/Debian</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Update system</span>
sudo<span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>apt<span class="w"> </span>upgrade<span class="w"> </span>-y

<span class="c1"># Install build tools</span>
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>build-essential<span class="w"> </span>cmake<span class="w"> </span>git
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>python3-dev<span class="w"> </span>libfftw3-dev
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>libusb-1.0-0-dev<span class="w"> </span>pkg-config

<span class="c1"># Install SDR libraries</span>
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>rtl-sdr<span class="w"> </span>librtlsdr-dev
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>hackrf<span class="w"> </span>libhackrf-dev
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>soapysdr-tools<span class="w"> </span>libsoapysdr-dev
</pre></div>
</div>
<p><strong>CentOS/RHEL</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Update system</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>update<span class="w"> </span>-y

<span class="c1"># Install development tools</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>groupinstall<span class="w"> </span>-y<span class="w"> </span><span class="s2">&quot;Development Tools&quot;</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>cmake<span class="w"> </span>git<span class="w"> </span>python3-devel
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>fftw-devel<span class="w"> </span>libusb1-devel

<span class="c1"># Install EPEL for additional packages</span>
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>epel-release
sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>rtl-sdr<span class="w"> </span>rtl-sdr-devel
</pre></div>
</div>
</section>
<section id="install-geminisdr">
<h3>5. Install GeminiSDR<a class="headerlink" href="#install-geminisdr" title="Link to this heading"></a></h3>
<p>Clone and install with CUDA optimizations:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone repository</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/your-org/geminisdr.git
<span class="nb">cd</span><span class="w"> </span>geminisdr

<span class="c1"># Install with CUDA support</span>
<span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.

<span class="c1"># Install additional dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements-cuda.txt
</pre></div>
</div>
</section>
<section id="configure-for-cuda-cluster">
<h3>6. Configure for CUDA Cluster<a class="headerlink" href="#configure-for-cuda-cluster" title="Link to this heading"></a></h3>
<p>Create CUDA-optimized configuration:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copy CUDA configuration template</span>
cp<span class="w"> </span>conf/hardware/cuda_cluster.yaml<span class="w"> </span>conf/local_config.yaml

<span class="c1"># Edit for your cluster setup</span>
nano<span class="w"> </span>conf/local_config.yaml
</pre></div>
</div>
<p>Example CUDA cluster configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># CUDA cluster optimized configuration</span>
<span class="nt">hardware</span><span class="p">:</span>
<span class="w">  </span><span class="nt">device_preference</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;cuda&quot;</span>
<span class="w">  </span><span class="nt">memory_optimization</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;aggressive&quot;</span>
<span class="w">  </span><span class="nt">multi_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">gpu_memory_fraction</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.9</span>

<span class="nt">ml</span><span class="p">:</span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w">  </span><span class="c1"># Auto-optimize for GPU memory</span>
<span class="w">  </span><span class="nt">precision</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mixed&quot;</span><span class="w">  </span><span class="c1"># Mixed precision training</span>
<span class="w">  </span><span class="nt">model_cache_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6</span><span class="w">  </span><span class="c1"># Large cache for multiple GPUs</span>
<span class="w">  </span><span class="nt">distributed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">gradient_checkpointing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w">  </span><span class="c1"># Disable for performance</span>

<span class="nt">performance</span><span class="p">:</span>
<span class="w">  </span><span class="nt">memory_threshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.85</span>
<span class="w">  </span><span class="nt">auto_optimize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">profiling_enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">benchmark_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>

<span class="nt">distributed</span><span class="p">:</span>
<span class="w">  </span><span class="nt">backend</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nccl&quot;</span><span class="w">  </span><span class="c1"># NVIDIA Collective Communications Library</span>
<span class="w">  </span><span class="nt">init_method</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;env://&quot;</span>
<span class="w">  </span><span class="nt">world_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">  </span><span class="c1"># Set based on number of nodes</span>
<span class="w">  </span><span class="nt">rank</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">  </span><span class="c1"># Set based on node rank</span>
</pre></div>
</div>
</section>
</section>
<section id="multi-gpu-configuration">
<h2>Multi-GPU Configuration<a class="headerlink" href="#multi-gpu-configuration" title="Link to this heading"></a></h2>
<section id="single-node-multi-gpu">
<h3>Single Node Multi-GPU<a class="headerlink" href="#single-node-multi-gpu" title="Link to this heading"></a></h3>
<p>Configure for multiple GPUs on single node:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multi-GPU training setup</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup_multi_gpu</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Setup multi-GPU training on single node.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Initialize distributed training</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
            <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span>
            <span class="n">world_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span>
            <span class="n">rank</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;LOCAL_RANK&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="c1"># Set device</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multi-node-configuration">
<h3>Multi-Node Configuration<a class="headerlink" href="#multi-node-configuration" title="Link to this heading"></a></h3>
<p>For distributed training across multiple nodes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># SLURM job script example</span>
<span class="c1">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=geminisdr-training</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="c1">#SBATCH --gres=gpu:4</span>
<span class="c1">#SBATCH --time=24:00:00</span>

<span class="c1"># Load modules</span>
module<span class="w"> </span>load<span class="w"> </span>cuda/11.8
module<span class="w"> </span>load<span class="w"> </span>python/3.9

<span class="c1"># Activate environment</span>
<span class="nb">source</span><span class="w"> </span>geminisdr-cuda-env/bin/activate

<span class="c1"># Set distributed training environment</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>scontrol<span class="w"> </span>show<span class="w"> </span>hostnames<span class="w"> </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">29500</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="nv">$SLURM_NTASKS</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RANK</span><span class="o">=</span><span class="nv">$SLURM_PROCID</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LOCAL_RANK</span><span class="o">=</span><span class="nv">$SLURM_LOCALID</span>

<span class="c1"># Run distributed training</span>
srun<span class="w"> </span>python<span class="w"> </span>train_distributed.py
</pre></div>
</div>
</section>
</section>
<section id="performance-optimization">
<h2>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Link to this heading"></a></h2>
<section id="cuda-optimizations">
<h3>CUDA Optimizations<a class="headerlink" href="#cuda-optimizations" title="Link to this heading"></a></h3>
<p>Enable CUDA-specific optimizations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Enable TensorFloat-32 for A100 GPUs</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Enable cuDNN benchmarking for consistent input sizes</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Enable cuDNN deterministic mode (if reproducibility needed)</span>
<span class="c1"># torch.backends.cudnn.deterministic = True</span>

<span class="c1"># Set memory allocation strategy</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">set_per_process_memory_fraction</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mixed-precision-training">
<h3>Mixed Precision Training<a class="headerlink" href="#mixed-precision-training" title="Link to this heading"></a></h3>
<p>Use automatic mixed precision for better performance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradScaler</span><span class="p">,</span> <span class="n">autocast</span>

<span class="c1"># Initialize scaler for mixed precision</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="c1"># Training loop with mixed precision</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Scale loss and backward pass</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="memory-management">
<h3>Memory Management<a class="headerlink" href="#memory-management" title="Link to this heading"></a></h3>
<p>Optimize GPU memory usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">geminisdr.core.memory_manager</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryManager</span>

<span class="c1"># Initialize with CUDA-specific settings</span>
<span class="n">memory_manager</span> <span class="o">=</span> <span class="n">MemoryManager</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Monitor GPU memory</span>
<span class="k">def</span><span class="w"> </span><span class="nf">print_gpu_memory</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
        <span class="n">allocated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="n">reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">allocated</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB allocated, </span><span class="si">{</span><span class="n">reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB reserved&quot;</span><span class="p">)</span>

<span class="c1"># Use memory-efficient training</span>
<span class="k">with</span> <span class="n">memory_manager</span><span class="o">.</span><span class="n">memory_efficient_context</span><span class="p">():</span>
    <span class="c1"># Training code here</span>
    <span class="k">pass</span>
</pre></div>
</div>
</section>
</section>
<section id="monitoring-and-profiling">
<h2>Monitoring and Profiling<a class="headerlink" href="#monitoring-and-profiling" title="Link to this heading"></a></h2>
<section id="gpu-monitoring">
<h3>GPU Monitoring<a class="headerlink" href="#gpu-monitoring" title="Link to this heading"></a></h3>
<p>Monitor GPU usage during training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Monitor GPU usage</span>
nvidia-smi<span class="w"> </span>-l<span class="w"> </span><span class="m">1</span>

<span class="c1"># Detailed GPU monitoring</span>
nvidia-smi<span class="w"> </span>dmon<span class="w"> </span>-s<span class="w"> </span>pucvmet<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span>

<span class="c1"># Monitor specific processes</span>
nvidia-smi<span class="w"> </span>pmon<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="performance-profiling">
<h3>Performance Profiling<a class="headerlink" href="#performance-profiling" title="Link to this heading"></a></h3>
<p>Profile CUDA kernels and memory usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.profiler</span>

<span class="c1"># Profile training with CUDA events</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="s1">&#39;./log/profiler&#39;</span><span class="p">),</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># Training step</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="cluster-job-management">
<h2>Cluster Job Management<a class="headerlink" href="#cluster-job-management" title="Link to this heading"></a></h2>
<section id="slurm-integration">
<h3>SLURM Integration<a class="headerlink" href="#slurm-integration" title="Link to this heading"></a></h3>
<p>Example SLURM job scripts:</p>
<p><strong>Single GPU Job</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=geminisdr-single</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --gres=gpu:1</span>
<span class="c1">#SBATCH --mem=32G</span>
<span class="c1">#SBATCH --time=12:00:00</span>

<span class="nb">source</span><span class="w"> </span>geminisdr-cuda-env/bin/activate
python<span class="w"> </span>train_single_gpu.py
</pre></div>
</div>
<p><strong>Multi-GPU Job</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=geminisdr-multi</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH --gres=gpu:4</span>
<span class="c1">#SBATCH --mem=128G</span>
<span class="c1">#SBATCH --time=24:00:00</span>

<span class="nb">source</span><span class="w"> </span>geminisdr-cuda-env/bin/activate

<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3
<span class="nb">export</span><span class="w"> </span><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="m">4</span>

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">4</span><span class="w"> </span>python<span class="w"> </span>train_multi_gpu.py
</pre></div>
</div>
</section>
<section id="job-monitoring">
<h3>Job Monitoring<a class="headerlink" href="#job-monitoring" title="Link to this heading"></a></h3>
<p>Monitor running jobs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check job status</span>
squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span>

<span class="c1"># Monitor job output</span>
tail<span class="w"> </span>-f<span class="w"> </span>slurm-&lt;jobid&gt;.out

<span class="c1"># Check GPU usage for job</span>
srun<span class="w"> </span>--jobid<span class="o">=</span>&lt;jobid&gt;<span class="w"> </span>--pty<span class="w"> </span>nvidia-smi
</pre></div>
</div>
</section>
</section>
<section id="testing-and-validation">
<h2>Testing and Validation<a class="headerlink" href="#testing-and-validation" title="Link to this heading"></a></h2>
<section id="cuda-functionality-tests">
<h3>CUDA Functionality Tests<a class="headerlink" href="#cuda-functionality-tests" title="Link to this heading"></a></h3>
<p>Verify CUDA functionality:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run CUDA-specific tests</span>
pytest<span class="w"> </span>tests/cuda/<span class="w"> </span>-v

<span class="c1"># Test multi-GPU functionality</span>
pytest<span class="w"> </span>tests/distributed/<span class="w"> </span>-v

<span class="c1"># Performance benchmarks</span>
python<span class="w"> </span>scripts/benchmark_cuda.py
</pre></div>
</div>
</section>
<section id="memory-and-performance-tests">
<h3>Memory and Performance Tests<a class="headerlink" href="#memory-and-performance-tests" title="Link to this heading"></a></h3>
<p>Test memory usage and performance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test GPU memory allocation</span>
<span class="n">python</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Test memory allocation</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Allocated: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> GB&#39;</span><span class="p">)</span>

<span class="c1"># Test computation</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Matrix multiplication successful&#39;</span><span class="p">)</span>

<span class="c1"># Cleanup</span>
<span class="k">del</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<span class="s2">&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h2>
<section id="common-cuda-issues">
<h3>Common CUDA Issues<a class="headerlink" href="#common-cuda-issues" title="Link to this heading"></a></h3>
<p><strong>Issue</strong>: CUDA out of memory</p>
<blockquote>
<div><p><strong>Solution</strong>: Reduce batch size or enable memory optimizations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reduce batch size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># Start small and increase</span>

<span class="c1"># Enable gradient checkpointing</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># Clear cache regularly</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
<p><strong>Issue</strong>: cuDNN version mismatch</p>
<blockquote>
<div><p><strong>Solution</strong>: Verify cuDNN installation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check cuDNN version</span>
cat<span class="w"> </span>/usr/local/cuda/include/cudnn_version.h<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>CUDNN_MAJOR<span class="w"> </span>-A<span class="w"> </span><span class="m">2</span>

<span class="c1"># Reinstall PyTorch with correct CUDA version</span>
pip<span class="w"> </span>uninstall<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu118
</pre></div>
</div>
</div></blockquote>
<p><strong>Issue</strong>: Multi-GPU training hangs</p>
<blockquote>
<div><p><strong>Solution</strong>: Check NCCL configuration:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set NCCL debug level</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NCCL_DEBUG</span><span class="o">=</span>INFO

<span class="c1"># Check network connectivity between nodes</span>
ping<span class="w"> </span>&lt;other-node-ip&gt;

<span class="c1"># Verify InfiniBand (if available)</span>
ibstat
</pre></div>
</div>
</div></blockquote>
<p><strong>Issue</strong>: Poor multi-GPU scaling</p>
<blockquote>
<div><p><strong>Solution</strong>: Optimize data loading and communication:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Increase data loader workers</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Use efficient communication backend</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>

<span class="c1"># Optimize batch size per GPU</span>
<span class="n">batch_size_per_gpu</span> <span class="o">=</span> <span class="n">total_batch_size</span> <span class="o">//</span> <span class="n">world_size</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="performance-benchmarks">
<h3>Performance Benchmarks<a class="headerlink" href="#performance-benchmarks" title="Link to this heading"></a></h3>
<p>Expected performance on different GPU configurations:</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">CUDA Performance Benchmarks</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>GPU Configuration</p></th>
<th class="head"><p>Training Speed</p></th>
<th class="head"><p>Inference Latency</p></th>
<th class="head"><p>Memory Usage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Single RTX 3080</p></td>
<td><p>150 samples/sec</p></td>
<td><p>5ms</p></td>
<td><p>8GB</p></td>
</tr>
<tr class="row-odd"><td><p>Single A100 40GB</p></td>
<td><p>300 samples/sec</p></td>
<td><p>3ms</p></td>
<td><p>16GB</p></td>
</tr>
<tr class="row-even"><td><p>4x A100 40GB</p></td>
<td><p>1000 samples/sec</p></td>
<td><p>3ms</p></td>
<td><p>64GB</p></td>
</tr>
<tr class="row-odd"><td><p>8x A100 80GB</p></td>
<td><p>1800 samples/sec</p></td>
<td><p>3ms</p></td>
<td><p>128GB</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading"></a></h2>
<section id="development-workflow">
<h3>Development Workflow<a class="headerlink" href="#development-workflow" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Start Small</strong>: Begin with single GPU, then scale up</p></li>
<li><p><strong>Profile Early</strong>: Use profiling tools to identify bottlenecks</p></li>
<li><p><strong>Monitor Resources</strong>: Keep track of GPU memory and utilization</p></li>
<li><p><strong>Version Control</strong>: Track configuration changes and results</p></li>
</ol>
</section>
<section id="production-deployment">
<h3>Production Deployment<a class="headerlink" href="#production-deployment" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Container Images</strong>: Use Docker/Singularity for reproducible environments</p></li>
<li><p><strong>Resource Scheduling</strong>: Use SLURM or Kubernetes for job management</p></li>
<li><p><strong>Monitoring</strong>: Implement comprehensive monitoring and alerting</p></li>
<li><p><strong>Backup</strong>: Regular backup of models and configurations</p></li>
</ol>
</section>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h2>
<p>After successful installation:</p>
<ol class="arabic simple">
<li><p><strong>Run Benchmarks</strong>: Establish performance baselines</p></li>
<li><p><strong>Optimize Configuration</strong>: Tune settings for your specific hardware</p></li>
<li><p><strong>Set Up Monitoring</strong>: Implement comprehensive monitoring</p></li>
<li><p><strong>Scale Up</strong>: Move to multi-node distributed training</p></li>
</ol>
<p>For advanced CUDA optimization:</p>
<ul class="simple">
<li><p><span class="xref std std-doc">../advanced/performance_tuning</span></p></li>
<li><p><span class="xref std std-doc">../advanced/distributed_training</span></p></li>
<li><p><span class="xref std std-doc">../../development/profiling</span></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="linux_vm.html" class="btn btn-neutral float-left" title="Linux VM Installation Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../troubleshooting.html" class="btn btn-neutral float-right" title="Troubleshooting Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, GeminiSDR Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>