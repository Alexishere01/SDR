"""
Metrics collection and monitoring system for GeminiSDR.

This module provides comprehensive metrics collection, anomaly detection,
and monitoring capabilities for system and ML performance tracking.
"""

import time
import threading
import psutil
import json
from typing import Dict, Any, Optional, List, Callable, Union, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, deque
from dataclasses import dataclass, asdict, field
from enum import Enum
from pathlib import Path
import statistics
import warnings

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from geminisdr.config.config_models import SystemConfig
from geminisdr.core.logging_manager import get_logger


class MetricType(Enum):
    """Types of metrics that can be collected."""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"


class AlertSeverity(Enum):
    """Alert severity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class Metric:
    """Individual metric data point."""
    name: str
    value: Union[int, float]
    metric_type: MetricType
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert metric to dictionary."""
        return {
            "name": self.name,
            "value": self.value,
            "type": self.metric_type.value,
            "timestamp": self.timestamp.isoformat(),
            "tags": self.tags,
            "metadata": self.metadata
        }


@dataclass
class Alert:
    """Alert generated by anomaly detection."""
    metric_name: str
    severity: AlertSeverity
    message: str
    value: Union[int, float]
    threshold: Union[int, float]
    timestamp: datetime
    tags: Dict[str, str] = field(default_factory=dict)
    resolved: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert alert to dictionary."""
        return {
            "metric_name": self.metric_name,
            "severity": self.severity.value,
            "message": self.message,
            "value": self.value,
            "threshold": self.threshold,
            "timestamp": self.timestamp.isoformat(),
            "tags": self.tags,
            "resolved": self.resolved
        }


@dataclass
class SystemMetrics:
    """System performance metrics snapshot."""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    disk_usage_percent: float
    gpu_memory_used_mb: Optional[float] = None
    gpu_memory_total_mb: Optional[float] = None
    gpu_utilization_percent: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


@dataclass
class MLMetrics:
    """ML training/inference metrics."""
    timestamp: datetime
    model_name: str
    operation: str  # training, inference, validation
    epoch: Optional[int] = None
    batch_size: Optional[int] = None
    loss: Optional[float] = None
    accuracy: Optional[float] = None
    learning_rate: Optional[float] = None
    duration: Optional[float] = None
    memory_usage_mb: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


class AnomalyDetector:
    """Simple anomaly detection for metrics."""
    
    def __init__(self, window_size: int = 100, threshold_multiplier: float = 2.0):
        """
        Initialize anomaly detector.
        
        Args:
            window_size: Size of rolling window for statistics
            threshold_multiplier: Multiplier for standard deviation threshold
        """
        self.window_size = window_size
        self.threshold_multiplier = threshold_multiplier
        self.metric_windows: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
        self.lock = threading.RLock()
    
    def add_value(self, metric_name: str, value: float) -> Optional[Alert]:
        """
        Add value and check for anomalies.
        
        Args:
            metric_name: Name of the metric
            value: Metric value
            
        Returns:
            Alert if anomaly detected, None otherwise
        """
        with self.lock:
            window = self.metric_windows[metric_name]
            window.append(value)
            
            # Need at least 10 values for meaningful statistics
            if len(window) < 10:
                return None
            
            # Calculate statistics
            values = list(window)
            mean = statistics.mean(values)
            stdev = statistics.stdev(values) if len(values) > 1 else 0
            
            # Check for anomaly
            threshold = self.threshold_multiplier * stdev
            if abs(value - mean) > threshold:
                severity = self._determine_severity(abs(value - mean), threshold)
                return Alert(
                    metric_name=metric_name,
                    severity=severity,
                    message=f"Anomaly detected: {metric_name} = {value:.2f} (mean: {mean:.2f}, threshold: {threshold:.2f})",
                    value=value,
                    threshold=threshold,
                    timestamp=datetime.now()
                )
            
            return None
    
    def _determine_severity(self, deviation: float, threshold: float) -> AlertSeverity:
        """Determine alert severity based on deviation magnitude."""
        ratio = deviation / threshold
        if ratio > 3:
            return AlertSeverity.CRITICAL
        elif ratio > 2:
            return AlertSeverity.HIGH
        elif ratio > 1.5:
            return AlertSeverity.MEDIUM
        else:
            return AlertSeverity.LOW


class MetricsCollector:
    """
    System metrics collection and monitoring.
    
    Collects system performance metrics, ML metrics, and provides
    anomaly detection and alerting capabilities.
    """
    
    def __init__(self, config: SystemConfig):
        """
        Initialize metrics collector.
        
        Args:
            config: System configuration
        """
        self.config = config
        self.logger = get_logger(__name__, config.logging)
        
        # Metrics storage
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.alerts: deque = deque(maxlen=1000)
        self.metrics_lock = threading.RLock()
        self.alerts_lock = threading.RLock()
        
        # Anomaly detection
        self.anomaly_detector = AnomalyDetector()
        
        # Monitoring state
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.monitoring_interval = 30  # seconds
        
        # Alert callbacks
        self.alert_callbacks: List[Callable[[Alert], None]] = []
        
        # Health check endpoints
        self.health_checks: Dict[str, Callable[[], bool]] = {}
        
        # Initialize system monitoring
        self._initialize_system_monitoring()
    
    def _initialize_system_monitoring(self) -> None:
        """Initialize system resource monitoring."""
        # Register default health checks
        self.register_health_check("system_memory", self._check_memory_health)
        self.register_health_check("system_cpu", self._check_cpu_health)
        self.register_health_check("disk_space", self._check_disk_health)
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            self.register_health_check("gpu_memory", self._check_gpu_health)
    
    def start_monitoring(self) -> None:
        """Start background metrics collection."""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="MetricsCollector"
        )
        self.monitoring_thread.start()
        self.logger.info("Metrics monitoring started")
    
    def stop_monitoring(self) -> None:
        """Stop background metrics collection."""
        self.monitoring_active = False
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=5)
        self.logger.info("Metrics monitoring stopped")
    
    def _monitoring_loop(self) -> None:
        """Background monitoring loop."""
        while self.monitoring_active:
            try:
                self.collect_system_metrics()
                time.sleep(self.monitoring_interval)
            except Exception as e:
                self.logger.log_error_with_context(e, operation="metrics_monitoring")
                time.sleep(self.monitoring_interval)
    
    def record_metric(self, name: str, value: Union[int, float], 
                     metric_type: MetricType = MetricType.GAUGE,
                     tags: Optional[Dict[str, str]] = None,
                     metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Record a metric value.
        
        Args:
            name: Metric name
            value: Metric value
            metric_type: Type of metric
            tags: Optional tags for the metric
            metadata: Optional metadata
        """
        metric = Metric(
            name=name,
            value=value,
            metric_type=metric_type,
            timestamp=datetime.now(),
            tags=tags or {},
            metadata=metadata or {}
        )
        
        with self.metrics_lock:
            self.metrics[name].append(metric)
        
        # Check for anomalies
        if metric_type in [MetricType.GAUGE, MetricType.TIMER]:
            alert = self.anomaly_detector.add_value(name, float(value))
            if alert:
                self._handle_alert(alert)
        
        self.logger.debug(f"Recorded metric: {name} = {value}", 
                         metric_name=name, metric_value=value, metric_type=metric_type.value)
    
    def record_counter(self, name: str, value: Union[int, float] = 1,
                      tags: Optional[Dict[str, str]] = None) -> None:
        """Record a counter metric."""
        self.record_metric(name, value, MetricType.COUNTER, tags)
    
    def record_gauge(self, name: str, value: Union[int, float],
                    tags: Optional[Dict[str, str]] = None) -> None:
        """Record a gauge metric."""
        self.record_metric(name, value, MetricType.GAUGE, tags)
    
    def record_timer(self, name: str, duration: float,
                    tags: Optional[Dict[str, str]] = None) -> None:
        """Record a timer metric."""
        self.record_metric(name, duration, MetricType.TIMER, tags)
    
    def collect_system_metrics(self) -> SystemMetrics:
        """
        Collect current system metrics.
        
        Returns:
            SystemMetrics snapshot
        """
        # CPU and memory
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # GPU metrics if available
        gpu_memory_used = None
        gpu_memory_total = None
        gpu_utilization = None
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            try:
                gpu_memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024  # MB
                gpu_utilization = torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else None
            except Exception as e:
                self.logger.warning(f"Failed to collect GPU metrics: {e}")
        
        metrics = SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_used_mb=memory.used / 1024 / 1024,
            memory_available_mb=memory.available / 1024 / 1024,
            disk_usage_percent=disk.percent,
            gpu_memory_used_mb=gpu_memory_used,
            gpu_memory_total_mb=gpu_memory_total,
            gpu_utilization_percent=gpu_utilization
        )
        
        # Record individual metrics
        self.record_gauge("system.cpu_percent", cpu_percent)
        self.record_gauge("system.memory_percent", memory.percent)
        self.record_gauge("system.memory_used_mb", memory.used / 1024 / 1024)
        self.record_gauge("system.disk_usage_percent", disk.percent)
        
        if gpu_memory_used is not None:
            self.record_gauge("system.gpu_memory_used_mb", gpu_memory_used)
            self.record_gauge("system.gpu_memory_percent", 
                            (gpu_memory_used / gpu_memory_total) * 100 if gpu_memory_total else 0)
        
        return metrics
    
    def record_ml_metrics(self, model_name: str, operation: str,
                         epoch: Optional[int] = None,
                         batch_size: Optional[int] = None,
                         loss: Optional[float] = None,
                         accuracy: Optional[float] = None,
                         learning_rate: Optional[float] = None,
                         duration: Optional[float] = None,
                         memory_usage_mb: Optional[float] = None) -> None:
        """
        Record ML training/inference metrics.
        
        Args:
            model_name: Name of the model
            operation: Operation type (training, inference, validation)
            epoch: Current epoch (for training)
            batch_size: Batch size used
            loss: Loss value
            accuracy: Accuracy value
            learning_rate: Learning rate
            duration: Operation duration in seconds
            memory_usage_mb: Memory usage in MB
        """
        ml_metrics = MLMetrics(
            timestamp=datetime.now(),
            model_name=model_name,
            operation=operation,
            epoch=epoch,
            batch_size=batch_size,
            loss=loss,
            accuracy=accuracy,
            learning_rate=learning_rate,
            duration=duration,
            memory_usage_mb=memory_usage_mb
        )
        
        # Record individual metrics with tags
        tags = {"model": model_name, "operation": operation}
        
        if loss is not None:
            self.record_gauge(f"ml.{operation}.loss", loss, tags)
        if accuracy is not None:
            self.record_gauge(f"ml.{operation}.accuracy", accuracy, tags)
        if duration is not None:
            self.record_timer(f"ml.{operation}.duration", duration, tags)
        if memory_usage_mb is not None:
            self.record_gauge(f"ml.{operation}.memory_mb", memory_usage_mb, tags)
        
        # Create log data without duplicating keys
        log_data = ml_metrics.to_dict()
        self.logger.info(f"ML metrics recorded for {model_name} {operation}", **log_data)
    
    def get_metrics(self, name: str, time_range: Optional[timedelta] = None) -> List[Metric]:
        """
        Get metrics by name and optional time range.
        
        Args:
            name: Metric name
            time_range: Time range from now (None for all)
            
        Returns:
            List of metrics
        """
        with self.metrics_lock:
            metrics = list(self.metrics.get(name, []))
        
        if time_range:
            cutoff = datetime.now() - time_range
            metrics = [m for m in metrics if m.timestamp >= cutoff]
        
        return metrics
    
    def get_metric_summary(self, name: str, time_range: Optional[timedelta] = None) -> Dict[str, Any]:
        """
        Get summary statistics for a metric.
        
        Args:
            name: Metric name
            time_range: Time range from now
            
        Returns:
            Summary statistics
        """
        metrics = self.get_metrics(name, time_range)
        
        if not metrics:
            return {"count": 0}
        
        values = [m.value for m in metrics]
        
        return {
            "count": len(values),
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "stdev": statistics.stdev(values) if len(values) > 1 else 0,
            "latest": values[-1] if values else None,
            "latest_timestamp": metrics[-1].timestamp.isoformat() if metrics else None
        }
    
    def register_alert_callback(self, callback: Callable[[Alert], None]) -> None:
        """Register callback for alert notifications."""
        self.alert_callbacks.append(callback)
    
    def _handle_alert(self, alert: Alert) -> None:
        """Handle generated alert."""
        with self.alerts_lock:
            self.alerts.append(alert)
        
        # Log alert
        self.logger.warning(f"Alert: {alert.message}",
                           alert_severity=alert.severity.value,
                           metric_name=alert.metric_name,
                           metric_value=alert.value,
                           threshold=alert.threshold)
        
        # Notify callbacks
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                self.logger.log_error_with_context(e, operation="alert_callback")
    
    def get_alerts(self, severity: Optional[AlertSeverity] = None,
                  time_range: Optional[timedelta] = None,
                  resolved: Optional[bool] = None) -> List[Alert]:
        """
        Get alerts with optional filtering.
        
        Args:
            severity: Filter by severity
            time_range: Filter by time range from now
            resolved: Filter by resolved status
            
        Returns:
            List of alerts
        """
        with self.alerts_lock:
            alerts = list(self.alerts)
        
        # Apply filters
        if severity:
            alerts = [a for a in alerts if a.severity == severity]
        
        if time_range:
            cutoff = datetime.now() - time_range
            alerts = [a for a in alerts if a.timestamp >= cutoff]
        
        if resolved is not None:
            alerts = [a for a in alerts if a.resolved == resolved]
        
        return alerts
    
    def register_health_check(self, name: str, check_func: Callable[[], bool]) -> None:
        """
        Register a health check function.
        
        Args:
            name: Health check name
            check_func: Function that returns True if healthy
        """
        self.health_checks[name] = check_func
    
    def run_health_checks(self) -> Dict[str, bool]:
        """
        Run all registered health checks.
        
        Returns:
            Dictionary of health check results
        """
        results = {}
        
        for name, check_func in self.health_checks.items():
            try:
                results[name] = check_func()
            except Exception as e:
                self.logger.log_error_with_context(e, health_check=name)
                results[name] = False
        
        return results
    
    def _check_memory_health(self) -> bool:
        """Check system memory health."""
        memory = psutil.virtual_memory()
        return memory.percent < (self.config.performance.memory_threshold * 100)
    
    def _check_cpu_health(self) -> bool:
        """Check CPU health."""
        cpu_percent = psutil.cpu_percent(interval=1)
        return cpu_percent < 90  # 90% CPU threshold
    
    def _check_disk_health(self) -> bool:
        """Check disk space health."""
        disk = psutil.disk_usage('/')
        return disk.percent < 90  # 90% disk usage threshold
    
    def _check_gpu_health(self) -> bool:
        """Check GPU memory health."""
        if not TORCH_AVAILABLE or not torch.cuda.is_available():
            return True
        
        try:
            memory_used = torch.cuda.memory_allocated()
            memory_total = torch.cuda.get_device_properties(0).total_memory
            usage_percent = (memory_used / memory_total) * 100
            return usage_percent < 90  # 90% GPU memory threshold
        except Exception:
            return False
    
    def export_metrics(self, filepath: str, time_range: Optional[timedelta] = None) -> None:
        """
        Export metrics to JSON file.
        
        Args:
            filepath: Output file path
            time_range: Time range to export (None for all)
        """
        export_data = {
            "timestamp": datetime.now().isoformat(),
            "time_range": time_range.total_seconds() if time_range else None,
            "metrics": {},
            "alerts": [alert.to_dict() for alert in self.get_alerts(time_range=time_range)]
        }
        
        with self.metrics_lock:
            for name, metric_deque in self.metrics.items():
                metrics = list(metric_deque)
                if time_range:
                    cutoff = datetime.now() - time_range
                    metrics = [m for m in metrics if m.timestamp >= cutoff]
                
                export_data["metrics"][name] = [m.to_dict() for m in metrics]
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        self.logger.info(f"Metrics exported to {filepath}")
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """
        Get data for monitoring dashboard.
        
        Returns:
            Dashboard data dictionary
        """
        # Get recent system metrics
        system_metrics = self.collect_system_metrics()
        
        # Get health check results
        health_checks = self.run_health_checks()
        
        # Get recent alerts
        recent_alerts = self.get_alerts(time_range=timedelta(hours=24), resolved=False)
        
        # Get metric summaries for key metrics
        key_metrics = [
            "system.cpu_percent",
            "system.memory_percent",
            "system.gpu_memory_percent"
        ]
        
        metric_summaries = {}
        for metric_name in key_metrics:
            summary = self.get_metric_summary(metric_name, timedelta(hours=1))
            if summary["count"] > 0:
                metric_summaries[metric_name] = summary
        
        return {
            "timestamp": datetime.now().isoformat(),
            "system_metrics": system_metrics.to_dict(),
            "health_checks": health_checks,
            "recent_alerts": [alert.to_dict() for alert in recent_alerts],
            "metric_summaries": metric_summaries,
            "overall_health": all(health_checks.values())
        }